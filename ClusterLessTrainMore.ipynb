{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional,BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils\n",
    "from importlib import reload\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HELLOAS\n",
      "HELLOAS\n"
     ]
    }
   ],
   "source": [
    "print(\"HELLOAS\")\n",
    "feature_list = ['PctChgClCl','sumPctChgClCl_2','sumPctChgClCl_3','sumPctChgClCl_4','sumPctChgClCl_5','sumPctChgClCl_6',\n",
    "                'PctChgVol','sumPctChgVol_2','sumPctChgVol_3','sumPctChgVol_4','sumPctChgVol_5','sumPctChgVol_6',\n",
    "                'PctChgVix','sumPctChgVix_2','sumPctChgVix_3','sumPctChgVix_4','sumPctChgVix_5','sumPctChgVix_6','rsi','Close','Volume']\n",
    "# feature_list = ['PctChgClCl','sumPctChgClCl_2','sumPctChgClCl_3','sumPctChgClCl_4','sumPctChgClCl_5','sumPctChgClCl_6','Close']\n",
    "n_steps = 25\n",
    "\n",
    "features = len(feature_list)\n",
    "run_name = datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "run_number = 0\n",
    "tstart = 2015\n",
    "tend = 2020\n",
    "\n",
    "# tickerList = ['aapl','spy','amd','qqq','amzn','meta','msft','f']\n",
    "tickerList = ['aapl','spy','amd','qqq']\n",
    "# tickerList = ['aapl']\n",
    "\n",
    "trend_features = ['Close','Volume']\n",
    "cluster_features = ['PctChgClCl','sumPctChgClCl_2','sumPctChgClCl_3','sumPctChgClCl_4','sumPctChgClCl_5','sumPctChgClCl_6']\n",
    "print(\"HELLOAS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = run_name+\"/\"+str(run_number)\n",
    "\n",
    "if not os.path.exists(run_path):\n",
    "    os.makedirs(run_path)\n",
    "\n",
    "# Construct file name\n",
    "file_name = os.path.join(run_path, \"results.txt\")\n",
    "\n",
    "# Create and open the run_number.txt file for writing\n",
    "\n",
    "run_info = \"\"\n",
    "run_info += \"Tickers: \" + str(tickerList) + \"\\n\"\n",
    "run_info += \"cluster_features: \" + str(cluster_features) + \"\\n\"\n",
    "run_info += \"n_steps: \" + str(n_steps)+ \"\\n\"\n",
    "run_info += \"Start: \" + str(tstart) + \" end: \" + str(tend) + \"\\n\\n\"\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(run_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.128184e+08, 2.571420e+08, 2.631884e+08, 1.604236e+08,\n",
       "       2.374580e+08, 2.147980e+08, 1.986032e+08, 2.683676e+08,\n",
       "       1.958264e+08, 2.400560e+08, 3.140532e+08, 1.995996e+08,\n",
       "       1.943036e+08, 2.151856e+08, 1.858592e+08, 2.224600e+08,\n",
       "       3.822748e+08, 5.859084e+08, 3.377456e+08, 3.349820e+08,\n",
       "       2.509564e+08, 2.076628e+08, 2.805988e+08, 1.689848e+08,\n",
       "       1.748264e+08])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5924, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2696, 25, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2696, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "reload(utils)\n",
    "\n",
    "# Initialize arrays as empty arrays with the necessary number of dimensions\n",
    "X_train, X_test = np.empty((0, n_steps, len(feature_list))), np.empty((0, n_steps, len(feature_list)))\n",
    "y_train, y_test = np.empty((0, 6)), np.empty((0, 6))\n",
    "\n",
    "for ticker in tickerList: \n",
    "    df = utils.prepareStockDf(ticker,tstart)\n",
    "    training_set, test_set = utils.train_test_split(df,tstart, tend, feature_list)\n",
    "\n",
    "    dataset_total = df.loc[:,feature_list]\n",
    "    test_set = dataset_total[len(dataset_total) - len(test_set) - n_steps :].values\n",
    "\n",
    "    # scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    # training_set = scaler.fit_transform(training_set)\n",
    "    # test_set = scaler.transform(test_set)\n",
    "\n",
    "    cur_X_train, cur_y_train = utils.split_sequence(training_set, n_steps)\n",
    "    cur_X_test, cur_y_test = utils.split_sequence(test_set, n_steps)\n",
    "\n",
    "    # Append cur_X_train, cur_X_test, cur_y_train, cur_y_test to the total arrays\n",
    "    X_train = np.concatenate((X_train, cur_X_train), axis=0)\n",
    "    X_test = np.concatenate((X_test, cur_X_test), axis=0)\n",
    "    y_train = np.concatenate((y_train, cur_y_train), axis=0)\n",
    "    y_test = np.concatenate((y_test, cur_y_test), axis=0)\n",
    "\n",
    "\n",
    "display(X_train[0,:,-1])\n",
    "display(y_train.shape)\n",
    "display(X_test.shape)\n",
    "display(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23942673, 0.01863814, 0.01935514, 0.12616499, 0.42293904,\n",
       "        0.43154138, 0.23369183, 0.30322595, 0.27311856, 0.05949832,\n",
       "        0.        , 0.1956991 , 0.25519742, 0.45949832, 0.5010755 ,\n",
       "        0.5096773 , 0.22580649, 0.66810012, 0.92544799, 0.800717  ,\n",
       "        0.90609285, 0.90752685, 0.97275952, 1.        , 0.92759844]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trend_feature_index = [feature_list.index(trend) for trend in trend_features]\n",
    "\n",
    "X_train = utils.scale_patterns(X_train,trend_feature_index)\n",
    "display(X_train[0:1,:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5924, 25, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.29968238, -0.54666594, -0.73175814, -0.57676085, -0.04383813,\n",
       "         0.16766101,  0.00840907,  0.39843351,  0.35732939, -0.07598302,\n",
       "        -0.5622445 , -0.30239192,  0.03736294,  0.21775969,  0.3121932 ,\n",
       "         0.60907623,  0.32229079,  0.64626286,  0.89363605,  0.46572227,\n",
       "         0.5434815 ,  0.53407611,  0.98339321,  0.42173664,  0.00535227]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_feature_index = [feature_list.index(cluster_feature) for cluster_feature in cluster_features]\n",
    "X_cluster = utils.create_Cluster_Seq(X_train,cluster_feature_index)\n",
    "display(X_cluster.shape)\n",
    "display(X_cluster[0:1,:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "n_clusters = math.ceil(math.sqrt(len(X_cluster))) // 3\n",
    "# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN\n",
    "\n",
    "km = TimeSeriesKMeans(n_clusters=n_clusters)\n",
    "\n",
    "labels = km.fit_predict(X_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5924, 25, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5924, 25, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(5924, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4963, 25, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4963, 25, 21)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4963, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4963,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reload(utils)\n",
    "threshold_factor = .5 # Adjust as necessary\n",
    "\n",
    "X_cluster_adj, X_train_adj, y_train_adj, labels_adj = utils.remove_outliers(X_cluster,X_train,y_train,labels,km,threshold_factor)\n",
    "display(X_cluster.shape)\n",
    "display(X_train.shape)\n",
    "display(y_train.shape)\n",
    "display(X_cluster_adj.shape)\n",
    "display(X_train_adj.shape)\n",
    "display(y_train_adj.shape)\n",
    "display(labels_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)\n",
    "utils.visualizeData(labels_adj,X_cluster_adj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster_assignments = km.predict(X_test)\n",
    "# set(cluster_assignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "visualDict = {'labels': labels, 'X_train':X_train}\n",
    "pickle_name = os.path.join(run_path, \"X_train.pkl\")\n",
    "\n",
    "with open(pickle_name, 'wb') as file:\n",
    "    pickle.dump(visualDict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22    532\n",
       "4     396\n",
       "3     304\n",
       "6     285\n",
       "9     273\n",
       "15    264\n",
       "24    239\n",
       "1     231\n",
       "8     227\n",
       "18    218\n",
       "13    195\n",
       "10    184\n",
       "14    179\n",
       "5     178\n",
       "19    172\n",
       "0     144\n",
       "12    143\n",
       "11    137\n",
       "21    117\n",
       "2     109\n",
       "7     105\n",
       "20     94\n",
       "17     93\n",
       "16     73\n",
       "23     71\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(y_train.shape) == 1:\n",
    "    df = pd.DataFrame({'label': labels_adj, 'target': y_train_adj})\n",
    "else:\n",
    "    # Convert the 2D y_train into a DataFrame\n",
    "    y_df = pd.DataFrame(y_train_adj, columns=[f'target_{i}' for i in range(y_train_adj.shape[1])])\n",
    "    df = pd.concat([pd.DataFrame({'label': labels_adj}), y_df], axis=1)\n",
    "\n",
    "df.head()\n",
    "display(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(X_train.shape)\n",
    "# display(y_train.mean())\n",
    "# display(cluster_X_train[0])\n",
    "\n",
    "# label_filter = 1\n",
    "# filtered_x_train, filtered_y_train = utils.filter_clusters(labels,label_filter,X_train_scaled,y_train_scaled)\n",
    "# filtered_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model_lstm = Sequential()\n",
    "\n",
    "    # Using Bidirectional LSTM for the first layer\n",
    "    model_lstm.add(Bidirectional(LSTM(units=100, activation='tanh', return_sequences=True), input_shape=(None, features)))\n",
    "    model_lstm.add(BatchNormalization())  # Batch normalization\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "\n",
    "    model_lstm.add(LSTM(units=250, activation='tanh', return_sequences=True))  # Changed number of units to 250\n",
    "    model_lstm.add(BatchNormalization())  # Batch normalization\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "\n",
    "    model_lstm.add(LSTM(units=250, activation='tanh', return_sequences=True))  # Changed number of units to 250\n",
    "    model_lstm.add(BatchNormalization())  # Batch normalization\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "\n",
    "    model_lstm.add(GRU(units=250, activation='tanh', return_sequences=True))  # Changed number of units to 250\n",
    "    model_lstm.add(BatchNormalization())  # Batch normalization\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "\n",
    "    model_lstm.add(GRU(units=100, activation='tanh', return_sequences=True))  # Changed number of units to 250\n",
    "    model_lstm.add(BatchNormalization())  # Batch normalization\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "\n",
    "    model_lstm.add(LSTM(units=100, activation='tanh'))  # Changed number of units to 100\n",
    "    model_lstm.add(BatchNormalization())  # Batch normalization\n",
    "    model_lstm.add(Dropout(0.2))\n",
    "\n",
    "    model_lstm.add(Dense(units=6))\n",
    "\n",
    "    # Using Adam optimizer with a learning rate scheduler\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=1, min_lr=1e-5)\n",
    "\n",
    "    model_lstm.compile(optimizer=optimizer, loss=\"mse\")\n",
    "    return model_lstm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lab in set(labels_adj): \n",
    "    filtered_x_train, filtered_y_train = utils.filter_clusters(labels_adj, lab, X_train_adj, y_train_adj)\n",
    "\n",
    "    X_test = utils.scale_patterns(X_test,trend_feature_index)\n",
    "    # display(X_train[0:1,:,-1])\n",
    "    X_cluster_test = utils.create_Cluster_Seq(X_test,cluster_feature_index)\n",
    "    # display(X_cluster_test.shape)\n",
    "\n",
    "    cluster_assignments = km.predict(X_cluster_test)\n",
    "\n",
    "    # values, counts = np.unique(cluster_assignments, return_counts=True)\n",
    "\n",
    "    # # Display the results\n",
    "    # for value, count in zip(values, counts):\n",
    "    #     print(f\"Value: {value}, Count: {count}\")\n",
    "\n",
    "    X_cluster_test_adj,X_test_adj, y_test_adj, labels_test_adj = utils.remove_outliers(X_cluster_test,X_test,y_test,cluster_assignments,km,threshold_factor)\n",
    "\n",
    "    filtered_x_test, filtered_y_test = utils.filter_clusters(labels_test_adj, lab, X_test_adj, y_test_adj)\n",
    "    \n",
    "    # display(X_cluster_test_adj.shape)\n",
    "    # display(filtered_x_test.shape)\n",
    "    # display(filtered_y_test.shape)\n",
    "\n",
    "    if len(filtered_x_test) == 0:\n",
    "        continue\n",
    "\n",
    "    # display(filtered_x_train.shape)\n",
    "    model_lstm = create_model()\n",
    "    model_lstm.fit(filtered_x_train, filtered_y_train, batch_size=30, epochs=350, verbose=1)\n",
    "\n",
    "    \n",
    "\n",
    "    predicted_stock_price = model_lstm.predict(filtered_x_test)\n",
    "\n",
    "    # Assuming predicted_stock_price has two columns\n",
    "    results = pd.DataFrame({\n",
    "        'predicted_1': predicted_stock_price[:, 0],\n",
    "        'predicted_2': predicted_stock_price[:, 1],\n",
    "        'predicted_3': predicted_stock_price[:, 2],\n",
    "        'predicted_4': predicted_stock_price[:, 3],\n",
    "        'predicted_5': predicted_stock_price[:, 4],\n",
    "        'predicted_6': predicted_stock_price[:, 5],\n",
    "        'real_1': filtered_y_test[:, 0],\n",
    "        'real_2': filtered_y_test[:, 1],\n",
    "        'real_3': filtered_y_test[:, 2],\n",
    "        'real_4': filtered_y_test[:, 3],\n",
    "        'real_5': filtered_y_test[:, 4],\n",
    "        'real_6': filtered_y_test[:, 5]\n",
    "    })\n",
    "\n",
    "    # Assuming a correct prediction means both predicted features have the same sign as their corresponding real values\n",
    "    results['same_1'] = ((results['predicted_1'] > 0) & (results['real_1'] > 0)) | ((results['predicted_1'] < 0) & (results['real_1'] < 0))\n",
    "    results['same_2'] = ((results['predicted_2'] > 0) & (results['real_2'] > 0)) | ((results['predicted_2'] < 0) & (results['real_2'] < 0))\n",
    "    results['same_3'] = ((results['predicted_3'] > 0) & (results['real_3'] > 0)) | ((results['predicted_3'] < 0) & (results['real_3'] < 0))\n",
    "    results['same_4'] = ((results['predicted_4'] > 0) & (results['real_4'] > 0)) | ((results['predicted_4'] < 0) & (results['real_4'] < 0))\n",
    "    results['same_5'] = ((results['predicted_5'] > 0) & (results['real_5'] > 0)) | ((results['predicted_5'] < 0) & (results['real_5'] < 0))\n",
    "    results['same_6'] = ((results['predicted_6'] > 0) & (results['real_6'] > 0)) | ((results['predicted_6'] < 0) & (results['real_6'] < 0))\n",
    "    accuracy1 = results['same_1'].sum() / len(results) * 100\n",
    "    accuracy2 = results['same_2'].sum() / len(results) * 100\n",
    "    accuracy3 = results['same_3'].sum() / len(results) * 100\n",
    "    accuracy4 = results['same_4'].sum() / len(results) * 100\n",
    "    accuracy5 = results['same_5'].sum() / len(results) * 100\n",
    "    accuracy6 = results['same_6'].sum() / len(results) * 100\n",
    "\n",
    "    # Construct the string\n",
    "    output_string = (\n",
    "        \"Cluster Number: \" + str(lab) +\n",
    "        \" \\nAccuracy1D \" + str(accuracy1) + \" PredictedRet: \" + str(results['predicted_1'].mean()) + \" ActRet \" + str(results['real_1'].mean() ) +\n",
    "        \" \\nAccuracy2D \" + str(accuracy2) + \" PredictedRet: \" + str(results['predicted_2'].mean()) + \" ActRet \" + str(results['real_2'].mean() ) +\n",
    "        \" \\nAccuracy3D \" + str(accuracy3) + \" PredictedRet: \" + str(results['predicted_3'].mean()) + \" ActRet \" + str(results['real_3'].mean() ) +\n",
    "        \" \\nAccuracy4D \" + str(accuracy4) + \" PredictedRet: \" + str(results['predicted_4'].mean()) + \" ActRet \" + str(results['real_4'].mean() ) +\n",
    "        \" \\nAccuracy5D \" + str(accuracy5) + \" PredictedRet: \" + str(results['predicted_5'].mean()) + \" ActRet \" + str(results['real_5'].mean() ) +\n",
    "        \" \\nAccuracy6D \" + str(accuracy6) + \" PredictedRet: \" + str(results['predicted_6'].mean()) + \" ActRet \" + str(results['real_6'].mean() ) +\n",
    "        \" Test set length: \" + str(len(filtered_y_test)) + \"\\n\"\n",
    "    )\n",
    "\n",
    "    # Write the string to a file\n",
    "    with open(file_name, 'a') as f:\n",
    "        f.write(output_string)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
